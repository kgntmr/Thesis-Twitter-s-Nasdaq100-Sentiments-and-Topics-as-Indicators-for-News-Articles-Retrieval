{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "kf0WMTIt7_XW"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-chroma langchain-openai chroma langchainhub datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standart Libraries\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Web Scraping and HTTP Requests\n",
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "\n",
        "# Data Handling and Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets # For the metric\n",
        "\n",
        "# Langchain for RAG\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To experiment with the plain version of the model, just changing the line below is enough to display the difference.\n",
        "\n",
        "# qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MolCS8DpD4rY",
        "outputId": "245d365e-8a31-4675-894c-e3e2ec895790"
      },
      "outputs": [],
      "source": [
        "# Initialize the question-answering pipeline from the Fine-Tuned model\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"kgntmr/RoBERTa-SQuAD2.0-SubjQA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW8sy8AND4rZ",
        "outputId": "b1adf0e8-aa8a-4e0a-e05c-80073dfdf8d0"
      },
      "outputs": [],
      "source": [
        "# Get API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aDa0vwBLD4rZ"
      },
      "outputs": [],
      "source": [
        "# Define the WebBaseLoader class\n",
        "class WebBaseLoader:\n",
        "    # Constructor to initialize the WebBaseLoader object with web_paths and bs_kwargs\n",
        "    def __init__(self, web_paths, bs_kwargs):\n",
        "        self.web_paths = web_paths  # Stores a list of URLs to be processed\n",
        "        self.bs_kwargs = bs_kwargs  # Stores additional arguments for BeautifulSoup\n",
        "\n",
        "    # Method to load data from each web path and parse the HTML content\n",
        "    def load(self):\n",
        "        results = {}  # Dictionary to store the results of web scraping\n",
        "        for url in self.web_paths:  # Iterating over each URL in the web_paths list\n",
        "            try:\n",
        "                response = requests.get(url)  # Sending a GET request to the URL\n",
        "                if response.status_code == 200:  # Checking if the request was successful\n",
        "                    # Parsing the HTML content with BeautifulSoup using the provided arguments\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser', **self.bs_kwargs)\n",
        "                    results[url] = soup.get_text()  # Extracting text from the parsed HTML and storing it in the results dictionary\n",
        "                else:\n",
        "                    results[url] = None  # Storing None if the response was unsuccessful\n",
        "            except requests.RequestException as e:  # Handling exceptions that may occur during the GET request\n",
        "                results[url] = str(e)  # Storing the exception message as the result for the URL\n",
        "        return results  # Returning the dictionary containing the results of the web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The URLs contains news articles according to the Semantic and Sentiment Analysis results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gOPHTrGwD4rZ"
      },
      "outputs": [],
      "source": [
        "# Function definition to fetch and return text content from specified website URLs using a given set of selector attributes\n",
        "def fetch_website_text(urls, selector_attrs):\n",
        "    # Creating a SoupStrainer object that filters out all unnecessary data except for elements matching the provided attributes\n",
        "    strainer = bs4.SoupStrainer(**selector_attrs)\n",
        "    # Initializing the WebBaseLoader with the URLs and the strainer object to parse only necessary parts of HTML\n",
        "    loader = WebBaseLoader(web_paths=urls, bs_kwargs={\"parse_only\": strainer})\n",
        "    # Calling the 'load' method from the WebBaseLoader instance to fetch and parse the web pages\n",
        "    return loader.load()\n",
        "\n",
        "# List of URLs from which to scrape data\n",
        "urls = [\n",
        "    \"https://www.theguardian.com/technology/2016/may/03/amazon-fresh-food-deliveries-understood-to-start-this-month\",\n",
        "    \"https://www.theguardian.com/media/2016/may/16/bbc-netflix-rival-itv-nbc-universal\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/28/amazon-most-profitable-quarter-sales-up-costs\",\n",
        "    \"https://www.theguardian.com/technology/2016/apr/26/amazon-kindle-oasis-review-luxury-e-reader\",\n",
        "    \"https://www.theguardian.com/environment/andes-to-the-amazon/2016/may/25/london-stock-exchange-amazon-deforestation\",\n",
        "    \"https://www.theguardian.com/media/2016/may/25/netflix-and-amazon-must-guarantee-20-of-content-is-european\",\n",
        "    \"https://www.theguardian.com/technology/2016/may/26/amazon-echo-virtual-assistant-child-privacy-law\",\n",
        "]\n",
        "# Dictionary specifying the attributes to filter HTML elements using SoupStrainer\n",
        "selector_attrs = {\"class\": \"article-body-commercial-selector\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U066izP1Sy5q"
      },
      "source": [
        "### The function fetch_website_text is now ready to be called with the list of URLs and selector attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dhkRHILWD4ra"
      },
      "outputs": [],
      "source": [
        "# Initialize the WebBaseLoader with URLs and BeautifulSoup keyword arguments\n",
        "loader = WebBaseLoader(urls, {\"parse_only\": bs4.SoupStrainer(**selector_attrs)})\n",
        "\n",
        "# Load the content from the specified URLs\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DGu7hfy2D4ra"
      },
      "outputs": [],
      "source": [
        "# Definition of the RecursiveCharacterTextSplitter class\n",
        "class RecursiveCharacterTextSplitter:\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        self.chunk_size = chunk_size  # The number of characters in each text chunk\n",
        "        self.chunk_overlap = chunk_overlap  # The number of characters each chunk overlaps with the next\n",
        "\n",
        "    def split_document(self, text):\n",
        "        return [text[i:i + self.chunk_size] for i in range(0, len(text), self.chunk_size - self.chunk_overlap)]\n",
        "\n",
        "    def split_documents(self, documents):\n",
        "        splits = []  # List to hold all chunks from all documents\n",
        "        for doc in documents:  # Iterating over each document in the provided list\n",
        "            if isinstance(doc, str):\n",
        "                text = doc  # Directly assigns the document to text if it is a string\n",
        "            else:\n",
        "                text = getattr(doc, 'page_content', '')  # Attempts to fetch 'page_content' from the document object; defaults to empty string if not found\n",
        "            splits.extend(self.split_document(text))  # Adds the chunks from the current document to the splits list\n",
        "        return splits  # Returns the list of all chunks from all documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dDJtS21YTF6m"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of RecursiveCharacterTextSplitter with a chunk size of 1000 characters and an overlap of 200 characters\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# Splitting a list of documents into smaller, overlapping chunks to maintain context between sections\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Initializing a vector store to enable semantic search capabilities using embeddings from OpenAI\n",
        "vectorstore = Chroma.from_texts(texts=splits, embedding=OpenAIEmbeddings())\n",
        "# Creating a retriever from the vector store for efficient information retrieval\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Retrieving a pre-defined prompt designed for use with language models in a retrieval-augmented generation setup\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to format strings from a list of documents into a single string\n",
        "def format_strings(documents):\n",
        "    formatted_documents = []  # List to hold formatted documents\n",
        "    for doc in documents:  # Iterate through each document in the input list\n",
        "        if isinstance(doc, str):\n",
        "            formatted_documents.append(doc)  # Add the string directly if the document is a string\n",
        "        elif isinstance(doc, dict):\n",
        "            # If the document is a dictionary, retrieve the value of 'page_content', defaulting to an empty string if not found\n",
        "            formatted_documents.append(doc.get('page_content', ''))\n",
        "        else:\n",
        "            # Append an empty string if the document is neither a string nor a dictionary\n",
        "            formatted_documents.append('')\n",
        "    # Join all formatted documents into a single string, separated by two newlines\n",
        "    return \"\\n\\n\".join(formatted_documents)\n",
        "\n",
        "# Usage of the function to format a list of documents\n",
        "formatted_context = format_strings(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the User Interface for RAG based on the user's queries\n",
        "def user_interface():\n",
        "    while True:\n",
        "        # User input for the question\n",
        "        question = input(\"Ask a question about Amazon between 25 April and 15 June 2016 (or type 'exit' to quit): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "        # Process the user's question using the retrieval-augmented generation pipeline\n",
        "        response = rag_answer(question, formatted_context)\n",
        "        print(\"Answer:\", response)\n",
        "\n",
        "# Function to generate answers using the RAG pipeline and provided context\n",
        "def rag_answer(question, context):\n",
        "    # Generate answer using the RAG pipeline\n",
        "    answer = qa_pipeline(question=question, context=context)\n",
        "    return answer['answer']  # Return only the 'answer' part of the result\n",
        "\n",
        "# # Main function to handle the application's execution flow\n",
        "# if __name__ == \"__main__\":\n",
        "#     user_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment\n",
        "\n",
        "#### Here is a brief summary of the experiment on Fine-Tuning and the RAG System:\n",
        "\n",
        "##### Fine-Tuning:\n",
        "\n",
        "- Before fine-tuning, the QA model had a prediction of 2% on EM and 9.41% on the F1 score.\n",
        "- After fine-tuning the SubjQA train data, the QA model was tested on unseen data using the SubjQA Test Dataset.\n",
        "- Fine-tuning resulted in a performance improvement to 62.52% EM and 64.62% F1 score on the test data.\n",
        "\n",
        "##### RAG System:\n",
        "\n",
        "- In this RAG system, from this point, the model is tested with the same dataset and metric used in the fine-tuning stage.\n",
        "- The plain version of the model which \"deepset/roberta-base-squad2\" had a prediction of 4.5% EM Score and 24.26% F1 Score\n",
        "- The fine-tuned version of the model resulted in a 17% EM score and a 21.85% F1 score, showing an improvement in the EM score but a decrease in the F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Test the RAG System\n",
        "- The code below is exactly the same as in the fine-tuning stage. The point at which the code starts to change will be indicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('subjqa-test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=data[['question','human_ans_indices','review','human_ans_spans']]\n",
        "data['id']=np.linspace(0,len(data)-1,len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>human_ans_indices</th>\n",
              "      <th>review</th>\n",
              "      <th>human_ans_spans</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Is this storyline interesting or strong?</td>\n",
              "      <td>(1850, 1886)</td>\n",
              "      <td>Spoilers thar be, Maytees.Is a man created by ...</td>\n",
              "      <td>just barely above mildly interesting</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is this storyline interesting or strong?</td>\n",
              "      <td>(240, 249)</td>\n",
              "      <td>Spoilers thar be, Maytees.Is a man created by ...</td>\n",
              "      <td>important</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is the sound of the movie a reason to recommen...</td>\n",
              "      <td>(111, 135)</td>\n",
              "      <td>This was my first bluray of a Disney classic a...</td>\n",
              "      <td>sound were crystal clear</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is the sound of the movie a reason to recommen...</td>\n",
              "      <td>(394, 408)</td>\n",
              "      <td>This was my first bluray of a Disney classic a...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do you rate the sound?</td>\n",
              "      <td>(5806, 5820)</td>\n",
              "      <td>In the realm of big Hollywood filmmaking, a fe...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577</th>\n",
              "      <td>How is it act ?</td>\n",
              "      <td>(539, 561)</td>\n",
              "      <td>del Toro's visual imagination is present in Pa...</td>\n",
              "      <td>the dialogue is clunky</td>\n",
              "      <td>577.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>How is the view?</td>\n",
              "      <td>(2034, 2048)</td>\n",
              "      <td>Now I am not a religious person, even tough I ...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>578.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>How is the view?</td>\n",
              "      <td>(2034, 2048)</td>\n",
              "      <td>Now I am not a religious person, even tough I ...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>579.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>How is the costume?</td>\n",
              "      <td>(26, 55)</td>\n",
              "      <td>The sets are spectucular, the costumes are so ...</td>\n",
              "      <td>the costumes are so authentic</td>\n",
              "      <td>580.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>How is the costume?</td>\n",
              "      <td>(26, 55)</td>\n",
              "      <td>The sets are spectucular, the costumes are so ...</td>\n",
              "      <td>the costumes are so authentic</td>\n",
              "      <td>581.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>582 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              question human_ans_indices  \\\n",
              "0             Is this storyline interesting or strong?      (1850, 1886)   \n",
              "1             Is this storyline interesting or strong?        (240, 249)   \n",
              "2    Is the sound of the movie a reason to recommen...        (111, 135)   \n",
              "3    Is the sound of the movie a reason to recommen...        (394, 408)   \n",
              "4                           How do you rate the sound?      (5806, 5820)   \n",
              "..                                                 ...               ...   \n",
              "577                                    How is it act ?        (539, 561)   \n",
              "578                                   How is the view?      (2034, 2048)   \n",
              "579                                   How is the view?      (2034, 2048)   \n",
              "580                                How is the costume?          (26, 55)   \n",
              "581                                How is the costume?          (26, 55)   \n",
              "\n",
              "                                                review  \\\n",
              "0    Spoilers thar be, Maytees.Is a man created by ...   \n",
              "1    Spoilers thar be, Maytees.Is a man created by ...   \n",
              "2    This was my first bluray of a Disney classic a...   \n",
              "3    This was my first bluray of a Disney classic a...   \n",
              "4    In the realm of big Hollywood filmmaking, a fe...   \n",
              "..                                                 ...   \n",
              "577  del Toro's visual imagination is present in Pa...   \n",
              "578  Now I am not a religious person, even tough I ...   \n",
              "579  Now I am not a religious person, even tough I ...   \n",
              "580  The sets are spectucular, the costumes are so ...   \n",
              "581  The sets are spectucular, the costumes are so ...   \n",
              "\n",
              "                          human_ans_spans     id  \n",
              "0    just barely above mildly interesting    0.0  \n",
              "1                               important    1.0  \n",
              "2                sound were crystal clear    2.0  \n",
              "3                          ANSWERNOTFOUND    3.0  \n",
              "4                          ANSWERNOTFOUND    4.0  \n",
              "..                                    ...    ...  \n",
              "577                the dialogue is clunky  577.0  \n",
              "578                        ANSWERNOTFOUND  578.0  \n",
              "579                        ANSWERNOTFOUND  579.0  \n",
              "580         the costumes are so authentic  580.0  \n",
              "581         the costumes are so authentic  581.0  \n",
              "\n",
              "[582 rows x 5 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "int(data.iloc[0].human_ans_indices.split('(')[1].split(',')[0])\n",
        "float(data.iloc[0].human_ans_indices.split('(')[1].split(',')[1].split(' ')[1].split(')')[0])\n",
        "data['answers']=data['human_ans_spans']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract answer data and adds it to a new column\n",
        "for i in range(0,len(data)):\n",
        "  answer1={}\n",
        "  si=int(data.iloc[i].human_ans_indices.split('(')[1].split(',')[0])\n",
        "  ei=int(data.iloc[i].human_ans_indices.split('(')[1].split(',')[1].split(' ')[1].split(')')[0])\n",
        "  answer1['text']=[data.iloc[i].review[si:ei]]\n",
        "  answer1['answer_start']=[si]\n",
        "  data.at[i, 'answers']=answer1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.columns=['question', 'human_ans_indices', 'context', 'human_ans_spans', 'id',\n",
        "       'answers']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>human_ans_indices</th>\n",
              "      <th>context</th>\n",
              "      <th>human_ans_spans</th>\n",
              "      <th>id</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Is this storyline interesting or strong?</td>\n",
              "      <td>(1850, 1886)</td>\n",
              "      <td>Spoilers thar be, Maytees.Is a man created by ...</td>\n",
              "      <td>just barely above mildly interesting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'text': ['just barely above mildly interestin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Is this storyline interesting or strong?</td>\n",
              "      <td>(240, 249)</td>\n",
              "      <td>Spoilers thar be, Maytees.Is a man created by ...</td>\n",
              "      <td>important</td>\n",
              "      <td>1.0</td>\n",
              "      <td>{'text': ['important'], 'answer_start': [240]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is the sound of the movie a reason to recommen...</td>\n",
              "      <td>(111, 135)</td>\n",
              "      <td>This was my first bluray of a Disney classic a...</td>\n",
              "      <td>sound were crystal clear</td>\n",
              "      <td>2.0</td>\n",
              "      <td>{'text': ['sound were crystal clear'], 'answer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is the sound of the movie a reason to recommen...</td>\n",
              "      <td>(394, 408)</td>\n",
              "      <td>This was my first bluray of a Disney classic a...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>3.0</td>\n",
              "      <td>{'text': ['ANSWERNOTFOUND'], 'answer_start': [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do you rate the sound?</td>\n",
              "      <td>(5806, 5820)</td>\n",
              "      <td>In the realm of big Hollywood filmmaking, a fe...</td>\n",
              "      <td>ANSWERNOTFOUND</td>\n",
              "      <td>4.0</td>\n",
              "      <td>{'text': ['ANSWERNOTFOUND'], 'answer_start': [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question human_ans_indices  \\\n",
              "0           Is this storyline interesting or strong?      (1850, 1886)   \n",
              "1           Is this storyline interesting or strong?        (240, 249)   \n",
              "2  Is the sound of the movie a reason to recommen...        (111, 135)   \n",
              "3  Is the sound of the movie a reason to recommen...        (394, 408)   \n",
              "4                         How do you rate the sound?      (5806, 5820)   \n",
              "\n",
              "                                             context  \\\n",
              "0  Spoilers thar be, Maytees.Is a man created by ...   \n",
              "1  Spoilers thar be, Maytees.Is a man created by ...   \n",
              "2  This was my first bluray of a Disney classic a...   \n",
              "3  This was my first bluray of a Disney classic a...   \n",
              "4  In the realm of big Hollywood filmmaking, a fe...   \n",
              "\n",
              "                        human_ans_spans   id  \\\n",
              "0  just barely above mildly interesting  0.0   \n",
              "1                             important  1.0   \n",
              "2              sound were crystal clear  2.0   \n",
              "3                        ANSWERNOTFOUND  3.0   \n",
              "4                        ANSWERNOTFOUND  4.0   \n",
              "\n",
              "                                             answers  \n",
              "0  {'text': ['just barely above mildly interestin...  \n",
              "1     {'text': ['important'], 'answer_start': [240]}  \n",
              "2  {'text': ['sound were crystal clear'], 'answer...  \n",
              "3  {'text': ['ANSWERNOTFOUND'], 'answer_start': [...  \n",
              "4  {'text': ['ANSWERNOTFOUND'], 'answer_start': [...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the metric\n",
        "metric = datasets.load_metric('squad') # As same as with Fine-Tuning Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The code remains unchanged up to this point from the fine-tuning stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare answers and predictions lists\n",
        "actual_answers = []\n",
        "predicted_answers = []\n",
        "\n",
        "for _, row in data.iterrows():\n",
        "    # Extract the first answer from the 'text' list in the 'answers' dictionary\n",
        "    actual_answer = row['answers']['text'][0]\n",
        "    if actual_answer != 'ANSWERNOTFOUND':\n",
        "        # Generate the answer using the RAG system\n",
        "        predicted_answer = rag_answer(row['question'], row['context'])\n",
        "        actual_answers.append(actual_answer.lower().strip())\n",
        "        predicted_answers.append(predicted_answer.lower().strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for metric calculation\n",
        "references = [{'id': str(i), 'answers': {'text': [ans], 'answer_start': [0]}} for i, ans in enumerate(actual_answers)]\n",
        "predictions = [{'id': str(i), 'prediction_text': ans} for i, ans in enumerate(predicted_answers)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the metrics\n",
        "results = metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Score: 17.0 %\n",
            "F1-Score: 21.850642921455368 %\n"
          ]
        }
      ],
      "source": [
        "# Print the Exact Match score and F1-Score\n",
        "print(\"Exact Match Score:\", results['exact_match'], \"%\")\n",
        "print(\"F1-Score:\", results['f1'], \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
